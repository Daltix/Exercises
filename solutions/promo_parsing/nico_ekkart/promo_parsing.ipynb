{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notebook to purse Testaankoop promotions.\n",
    "\"\"\"\n",
    "import os, re, enum, logging, decimal\n",
    "import pandas as pd\n",
    "import psycopg2 as pg\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger('parser')\n",
    "\n",
    "# TODO fill in your connection info.\n",
    "username = 'postgres'\n",
    "password = 'postgres'\n",
    "# TODO you should set this value to 5432!\n",
    "port = '5432'\n",
    "\n",
    "uri = 'postgres://{user}:{pw}@localhost:{port}/daltix'.format(user=username, pw=password, port=port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "\n",
    "Don't worry too much about the helper functions below, they might look scary but you don't need to understand them at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def count_trues(row):\n",
    "    count = 0\n",
    "    for t in row:\n",
    "        if t:\n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "#For igonoring several white spaces \n",
    "def multiply_whitespace(regex: str) -> str:\n",
    "    \"\"\"\n",
    "    Multiply whitespace in the given regex; replace any whitespace character with any sequence of whitespace characters.\n",
    "    :param regex:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s+', '\\s*', regex)\n",
    "\n",
    "def merge_into_array(data: pd.DataFrame, cols: list=None, drop_duplicates: bool=False) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Merge the given columns into a single column with as value an array containing all not-null elements of the\n",
    "    other columns. Does not alter data, but returns a new Series object.\n",
    "    Original items that were lists, will just be concatenated into longer lists.\n",
    "    :param data:\n",
    "    :param cols: The columns which to merge or concatenate into arrays. If None: concatenates all columns.\n",
    "    :param drop_duplicates: Whether to drop duplicate values from the resulting arrays.\n",
    "    :return: A new pd.Series object.\n",
    "    \"\"\"\n",
    "    # TODO: check status of this in Pandas 0.20.2\n",
    "    \"\"\"\n",
    "    Yet another case of Pandas' unfathomable inventiveness.\n",
    "    The easy version was:\n",
    "    data.apply(lambda x: [x[col] for col in cols if x[col] is not None], axis=1)\n",
    "    Or, in words, for each row return a list of all not-null elements and put that list in a single cell.\n",
    "    Easy, right? Well, turns out you can't do that if you have a timestamp as an index somewhere? What? Yeah.\n",
    "    Was on pandas 0.19.2.\n",
    "    \"\"\"\n",
    "    # Some preliminary checks: don't do anything if there is nothing to do...\n",
    "    if data.empty:\n",
    "        return pd.Series()\n",
    "    if cols is None:\n",
    "        cols = data.columns\n",
    "\n",
    "    original_index = data.index.names\n",
    "    indexless = pd.DataFrame(data.reset_index()[cols].copy())\n",
    "\n",
    "    def merge_or_concat(x):\n",
    "        try:\n",
    "            ret_list = []\n",
    "            for col in cols:\n",
    "                if isinstance(x[col], list):\n",
    "                    if drop_duplicates:\n",
    "                        ret_list += [y for y in x[col] if y not in ret_list and not pd.isnull(y)]\n",
    "                    else:\n",
    "                        ret_list += [y for y in x[col] if not pd.isnull(y)]\n",
    "                elif pd.isnull(x[col]):\n",
    "                    continue\n",
    "                else:\n",
    "                    if drop_duplicates:\n",
    "                        if x[col] not in ret_list:\n",
    "                            ret_list.append(x[col])\n",
    "                    else:\n",
    "                        ret_list.append(x[col])\n",
    "            return tuple(ret_list)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    listed = indexless.apply(merge_or_concat, axis=1)\n",
    "    tmp = data.reset_index().assign(listed=listed).set_index(original_index)\n",
    "    return tmp.listed.apply(lambda x: list(x) if len(x) > 0 else None)\n",
    "\n",
    "def concat_df_duplicate_cols(df1: pd.DataFrame, df2: pd.DataFrame, strategy: str='array', copy: bool=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Concatenate two dataframes columnwise, taking care of duplicate columns.\n",
    "    :param df1: First dataframe.\n",
    "    :param df2: Second dataframe.\n",
    "    :param strategy: Indicates the strategy. Allowed values are: 'array', 'exclusive', 'json_array'.\n",
    "    If set to 'array', will merge all not-null values into a list. If set to 'exclusive', will fail if there are\n",
    "    multiple not-null values per row. If set to 'json_array', will merge duplicate values into a JSON array string, but\n",
    "    *only if there is more than 1 result*, otherwise the single result will be returned. Duplicates will be dropped.\n",
    "    :return: The first dataframe, with the second concatenated to it.\n",
    "    \"\"\"\n",
    "    @enum.unique\n",
    "    class HelpEnum(enum.Enum):\n",
    "        ARRAY = 0\n",
    "        EXCLUSIVE = 1\n",
    "        JSON_ARRAY = 2\n",
    "\n",
    "    # Sanity check\n",
    "    if strategy.lower() == 'array':\n",
    "        strategy = HelpEnum.ARRAY\n",
    "    elif strategy.lower() == 'exclusive':\n",
    "        strategy = HelpEnum.EXCLUSIVE\n",
    "    elif strategy.lower() == 'json_array':\n",
    "        strategy = HelpEnum.JSON_ARRAY\n",
    "    else:\n",
    "        raise ValueError('Strategy must be either \"array\",  \"exclusive\" or \"json_array\".')\n",
    "\n",
    "    # Be safe!\n",
    "    if copy:\n",
    "        df1 = df1.copy()\n",
    "        df2 = df2.copy()\n",
    "    # Check if there's work to be done\n",
    "    overlap_cols = df1.columns.intersection(df2.columns)\n",
    "    if overlap_cols.empty:\n",
    "        # Easy peasy!\n",
    "        df1[df2.columns] = df2\n",
    "    else:\n",
    "        # There are overlapping columns.\n",
    "        # First take care of the ones without overlap\n",
    "        no_overlap = df2.columns.difference(overlap_cols)\n",
    "        df1[no_overlap] = df2[no_overlap]\n",
    "        # Then with overlap\n",
    "        for col in overlap_cols:\n",
    "            if strategy is HelpEnum.ARRAY:\n",
    "                df1[col] = merge_into_array(\n",
    "                    pd.DataFrame({'old': df1[col], 'new': df2[col]}),\n",
    "                    drop_duplicates=True)\n",
    "            elif strategy is HelpEnum.JSON_ARRAY:\n",
    "                df1[col] = merge_into_json_array(pd.DataFrame({'old': df1[col], 'new': df2[col]}), unwrap=True,\n",
    "                                                 drop_duplicates=True)\n",
    "            else:\n",
    "                # We only accept merging if there are no values together\n",
    "                valid = (df2[col].isnull() | df1[col].isnull()) | (df2[col] == df1[col])\n",
    "                if not valid.all():\n",
    "                    raise ValueError(\n",
    "                        \"Merging columns failed for column {col}. Appears twice, with conflicting data.\\n\"\n",
    "                        \"Data:{df1}\\n\"\n",
    "                        \"{df2}\"\n",
    "                            .format(col=col,\n",
    "                                    df1=df1[~valid].reset_index()[[\"shop\", \"location\", \"downloaded_on\", \"product_id\", col]],\n",
    "                                    df2=df2[~valid].reset_index()[[\"shop\", \"location\", \"downloaded_on\", \"product_id\", col]]),\n",
    "                        col\n",
    "                    )\n",
    "                else:\n",
    "                    \"\"\"\n",
    "                    Fuck pandas.\n",
    "                    If you don't do this mask inplace, it fails for timezoned timestamps. Somehow...\n",
    "                    Don't even ask about what's going on here. I wouldn't know.\n",
    "                    \"\"\"\n",
    "                    try:\n",
    "                        cop = df2[col].copy()\n",
    "                        cop.mask(cop.isnull(), df1[col], inplace=True)\n",
    "                        df1[col] = cop\n",
    "                    except ValueError:\n",
    "                        df1[col] = df2[col].mask(df2[col].isnull(), df1[col])\n",
    "    return df1\n",
    "\n",
    "def apply_regexes(data: pd.DataFrame, regexes: list, fail_limit: int=10, collate: bool=True):\n",
    "    \"\"\"\n",
    "    Apply regexes to the data, combining the extractions into a single DataFrame.\n",
    "    Use capturing groups with predefined names (see docs of create_promo).\n",
    "    Performs automatic price collation: The capturing groups 'xxx_int' and 'xxx_frac' will be combined into 'xxx' and\n",
    "    the original columns will be dropped, unless collate is set to False.\n",
    "    :param data: The data.\n",
    "    :param regexes: The regexes to apply.\n",
    "    :param fail_limit: The number of failed strings to print out. 0 means all failed lines will get printed.\n",
    "    Default is 10 because when Sentry is enabled we cannot log too many lines.\n",
    "    :param collate: Whether or not to automatically collate prices.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if len(regexes) == 0:\n",
    "        raise ValueError('Pass a list of regexes please!')\n",
    "        \n",
    "    extraction = pd.DataFrame()\n",
    "    if isinstance(data, pd.Series):\n",
    "        data = pd.DataFrame({'promos': data})\n",
    "    all_matches = data.isnull()\n",
    "    for regex in regexes:\n",
    "        regex = multiply_whitespace(regex=regex)\n",
    "        captures = (re.compile(regex).groups > 0)\n",
    "        for c in data.columns:\n",
    "            matches = data[c].str.match(regex).mask(data[c].isnull(), True).astype(bool)\n",
    "            all_matches[c] = all_matches[c] | matches\n",
    "            if captures:\n",
    "                e = data[c].str.extract(regex, expand=True)\n",
    "                try:\n",
    "                    extraction = concat_df_duplicate_cols(extraction, e, 'exclusive')\n",
    "                except ValueError as e:\n",
    "                    logger.exception(e.args[0])\n",
    "                    raise e\n",
    "    if not all_matches.all().all():\n",
    "        failed = data.stack()[~(all_matches.stack())].drop_duplicates().tolist()\n",
    "        if fail_limit != 0:\n",
    "            lines = failed[0:min(fail_limit, len(failed))]\n",
    "        else:\n",
    "            lines = failed\n",
    "\n",
    "        # Print as one line so that we have the faulty strings as is (easy to copy in regex 101).\n",
    "        logger.error('Not everything matches, listing ({}) unique failed lines:\\n{}'\n",
    "                     .format(len(lines), '\\n'.join(lines)))\n",
    "\n",
    "        unmatched = data.mask(all_matches, None)\n",
    "\n",
    "        def format_failures(b: pd.Series):\n",
    "            l = [c for c in b if c is not None]\n",
    "            if l:\n",
    "                return \"Could not match all strings. Unmatched:\\n{}\".format('\\n'.join(l))\n",
    "            else:\n",
    "                return None\n",
    "        extraction[\"errors\"] = unmatched.apply(format_failures, axis=1)\n",
    "\n",
    "    # Add the original promo strings\n",
    "    extraction['strings'] = data.promos\n",
    "    return extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "Load the promotion strings into a Dataframe. We only consider the first entry in the promo strings list as that is enough for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop</th>\n",
       "      <th>promo_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>collect and go</td>\n",
       "      <td>-25% vanaf 2 st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>collect and go</td>\n",
       "      <td>-1 € bij aankoop van 2 st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>collect and go</td>\n",
       "      <td>-2 € bij aankoop van 12 st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>collect and go</td>\n",
       "      <td>per 18 € 1,79/st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>collect and go</td>\n",
       "      <td>€ 4,27/st vanaf 4 st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>collect and go</td>\n",
       "      <td>€ 2,11/kg vanaf 3,0 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>collect and go</td>\n",
       "      <td>€ 15,20/kg vanaf 2 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>collect and go</td>\n",
       "      <td>-4,00 €bij aankoop van 2 st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>collect and go</td>\n",
       "      <td>-20,00% vanaf 2 st, vanaf 26/07/2017 tot en me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>carrefour</td>\n",
       "      <td>Voor 2, -25%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>carrefour</td>\n",
       "      <td>Koop 2, voor 6,78€ 5,08€</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ah</td>\n",
       "      <td>2 VOOR 3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ah</td>\n",
       "      <td>2E HALVE PRIJS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              shop                                       promo_string\n",
       "0   collect and go                                    -25% vanaf 2 st\n",
       "1   collect and go                          -1 € bij aankoop van 2 st\n",
       "2   collect and go                         -2 € bij aankoop van 12 st\n",
       "3   collect and go                                   per 18 € 1,79/st\n",
       "4   collect and go                               € 4,27/st vanaf 4 st\n",
       "5   collect and go                             € 2,11/kg vanaf 3,0 kg\n",
       "6   collect and go                              € 15,20/kg vanaf 2 kg\n",
       "7   collect and go                        -4,00 €bij aankoop van 2 st\n",
       "8   collect and go  -20,00% vanaf 2 st, vanaf 26/07/2017 tot en me...\n",
       "9        carrefour                                       Voor 2, -25%\n",
       "10       carrefour                           Koop 2, voor 6,78€ 5,08€\n",
       "11              ah                                        2 VOOR 3.00\n",
       "12              ah                                     2E HALVE PRIJS"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT shop, promo_string \n",
    "FROM promo_strings\n",
    "\"\"\"\n",
    "\n",
    "with pg.connect(uri) as connection:\n",
    "    promotions = pd.read_sql(query, connection)\n",
    "\n",
    "promotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['shop' 'location' 'downloaded_on' 'product_id'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-4e34900a2bd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;34m'(?i)(?P<amount>[\\d\\.,]+)E (?P<discount_string>HALVE PRIJS|GRATIS)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m ]\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mextraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_regexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpromotions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'promo_string'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Have a look at your output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4148c24c5328>\u001b[0m in \u001b[0;36mapply_regexes\u001b[0;34m(data, regexes, fail_limit, collate)\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                     \u001b[0mextraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_df_duplicate_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exclusive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4148c24c5328>\u001b[0m in \u001b[0;36mconcat_df_duplicate_cols\u001b[0;34m(df1, df2, strategy, copy)\u001b[0m\n\u001b[1;32m    128\u001b[0m                         \u001b[0;34m\"{df2}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                             .format(col=col,\n\u001b[0;32m--> 130\u001b[0;31m                                     \u001b[0mdf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shop\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"location\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"downloaded_on\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"product_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                                     df2=df2[~valid].reset_index()[[\"shop\", \"location\", \"downloaded_on\", \"product_id\", col]]),\n\u001b[1;32m    132\u001b[0m                         \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/daltix/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/daltix/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2175\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/daltix/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1269\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['shop' 'location' 'downloaded_on' 'product_id'] not in index\""
     ]
    }
   ],
   "source": [
    "# We define the following groups:\n",
    "# percent: the amount of % you get\n",
    "# amount: how many of the product do you have to buy in order to get the percentage/fixed discount\n",
    "# unit: what is the unit of the above amount?\n",
    "# start/end dates: from when till when is this promotion valid.\n",
    "# fixed_discount: amount of fixed discount you get\n",
    "# price: price per unit when the amount has been reached\n",
    "# discount_string: string representing the discount (halve prijs, gratis...)\n",
    "#\n",
    "# Standard unit = st\n",
    "\n",
    "\n",
    "regexes = [\n",
    "    '(?i)-(?P<percent>[\\d\\.,]+)% vanaf (?P<amount>[\\d\\.,]+) (?P<unit>st)(?:, vanaf (?P<start_date>[\\d/]+) tot en met (?P<end_date>[\\d/]+))?',\n",
    "    '(?i)-(?P<fixed_discount>[\\d\\.,]+) € bij aankoop van (?P<amount>[\\d\\.,]+) (?P<unit>\\w+)',\n",
    "    '(?i)per (?P<amount>[\\d\\.,]+) € (?P<price>[\\d\\.,]+)\\/(?P<unit>\\w+)',\n",
    "    '(?i)€ (?P<price>[\\d\\.,]+)\\/\\w+ vanaf (?P<amount>[\\d,]+) (?P<unit>\\w+)',\n",
    "    '(?i)Voor (?P<amount>[\\d\\.,]+), -(?P<percent>[\\d\\.,]+)%',\n",
    "    '(?i)Koop (?P<amount>[\\d\\.,]+), voor (?P<price>[\\d\\.,]+)€ (?:[\\d\\.,]+)€', # second price is probably already part of a next promotion\n",
    "    '(?i)(?P<amount>[\\d\\.,]+) VOOR  (?P<price>[\\d\\.,]+)',\n",
    "    '(?i)(?P<amount>[\\d\\.,]+)E (?P<discount_string>HALVE PRIJS|GRATIS)'\n",
    "]\n",
    "extraction = apply_regexes(promotions['promo_string'], regexes)\n",
    "\n",
    "# Have a look at your output.\n",
    "extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
