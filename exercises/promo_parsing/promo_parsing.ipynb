{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notebook to purse Testaankoop promotions.\n",
    "\"\"\"\n",
    "import os, re, enum, logging, decimal\n",
    "import pandas as pd\n",
    "import psycopg2 as pg\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger('parser')\n",
    "\n",
    "# TODO fill in your connection info.\n",
    "username = 'postgres'\n",
    "password = 'postgres'\n",
    "# TODO you should set this value to 5432!\n",
    "port = '55432'\n",
    "\n",
    "uri = 'postgres://{user}:{pw}@localhost:{port}/daltix'.format(user=username, pw=password, port=port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "\n",
    "Don't worry too much about the helper functions below, they might look scary but you don't need to understand them at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def count_trues(row):\n",
    "    count = 0\n",
    "    for t in row:\n",
    "        if t:\n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "#For igonoring several white spaces \n",
    "def multiply_whitespace(regex: str) -> str:\n",
    "    \"\"\"\n",
    "    Multiply whitespace in the given regex; replace any whitespace character with any sequence of whitespace characters.\n",
    "    :param regex:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s+', '\\s*', regex)\n",
    "\n",
    "def merge_into_array(data: pd.DataFrame, cols: list=None, drop_duplicates: bool=False) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Merge the given columns into a single column with as value an array containing all not-null elements of the\n",
    "    other columns. Does not alter data, but returns a new Series object.\n",
    "    Original items that were lists, will just be concatenated into longer lists.\n",
    "    :param data:\n",
    "    :param cols: The columns which to merge or concatenate into arrays. If None: concatenates all columns.\n",
    "    :param drop_duplicates: Whether to drop duplicate values from the resulting arrays.\n",
    "    :return: A new pd.Series object.\n",
    "    \"\"\"\n",
    "    # TODO: check status of this in Pandas 0.20.2\n",
    "    \"\"\"\n",
    "    Yet another case of Pandas' unfathomable inventiveness.\n",
    "    The easy version was:\n",
    "    data.apply(lambda x: [x[col] for col in cols if x[col] is not None], axis=1)\n",
    "    Or, in words, for each row return a list of all not-null elements and put that list in a single cell.\n",
    "    Easy, right? Well, turns out you can't do that if you have a timestamp as an index somewhere? What? Yeah.\n",
    "    Was on pandas 0.19.2.\n",
    "    \"\"\"\n",
    "    # Some preliminary checks: don't do anything if there is nothing to do...\n",
    "    if data.empty:\n",
    "        return pd.Series()\n",
    "    if cols is None:\n",
    "        cols = data.columns\n",
    "\n",
    "    original_index = data.index.names\n",
    "    indexless = pd.DataFrame(data.reset_index()[cols].copy())\n",
    "\n",
    "    def merge_or_concat(x):\n",
    "        try:\n",
    "            ret_list = []\n",
    "            for col in cols:\n",
    "                if isinstance(x[col], list):\n",
    "                    if drop_duplicates:\n",
    "                        ret_list += [y for y in x[col] if y not in ret_list and not pd.isnull(y)]\n",
    "                    else:\n",
    "                        ret_list += [y for y in x[col] if not pd.isnull(y)]\n",
    "                elif pd.isnull(x[col]):\n",
    "                    continue\n",
    "                else:\n",
    "                    if drop_duplicates:\n",
    "                        if x[col] not in ret_list:\n",
    "                            ret_list.append(x[col])\n",
    "                    else:\n",
    "                        ret_list.append(x[col])\n",
    "            return tuple(ret_list)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    listed = indexless.apply(merge_or_concat, axis=1)\n",
    "    tmp = data.reset_index().assign(listed=listed).set_index(original_index)\n",
    "    return tmp.listed.apply(lambda x: list(x) if len(x) > 0 else None)\n",
    "\n",
    "def concat_df_duplicate_cols(df1: pd.DataFrame, df2: pd.DataFrame, strategy: str='array', copy: bool=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Concatenate two dataframes columnwise, taking care of duplicate columns.\n",
    "    :param df1: First dataframe.\n",
    "    :param df2: Second dataframe.\n",
    "    :param strategy: Indicates the strategy. Allowed values are: 'array', 'exclusive', 'json_array'.\n",
    "    If set to 'array', will merge all not-null values into a list. If set to 'exclusive', will fail if there are\n",
    "    multiple not-null values per row. If set to 'json_array', will merge duplicate values into a JSON array string, but\n",
    "    *only if there is more than 1 result*, otherwise the single result will be returned. Duplicates will be dropped.\n",
    "    :return: The first dataframe, with the second concatenated to it.\n",
    "    \"\"\"\n",
    "    @enum.unique\n",
    "    class HelpEnum(enum.Enum):\n",
    "        ARRAY = 0\n",
    "        EXCLUSIVE = 1\n",
    "        JSON_ARRAY = 2\n",
    "\n",
    "    # Sanity check\n",
    "    if strategy.lower() == 'array':\n",
    "        strategy = HelpEnum.ARRAY\n",
    "    elif strategy.lower() == 'exclusive':\n",
    "        strategy = HelpEnum.EXCLUSIVE\n",
    "    elif strategy.lower() == 'json_array':\n",
    "        strategy = HelpEnum.JSON_ARRAY\n",
    "    else:\n",
    "        raise ValueError('Strategy must be either \"array\",  \"exclusive\" or \"json_array\".')\n",
    "\n",
    "    # Be safe!\n",
    "    if copy:\n",
    "        df1 = df1.copy()\n",
    "        df2 = df2.copy()\n",
    "    # Check if there's work to be done\n",
    "    overlap_cols = df1.columns.intersection(df2.columns)\n",
    "    if overlap_cols.empty:\n",
    "        # Easy peasy!\n",
    "        df1[df2.columns] = df2\n",
    "    else:\n",
    "        # There are overlapping columns.\n",
    "        # First take care of the ones without overlap\n",
    "        no_overlap = df2.columns.difference(overlap_cols)\n",
    "        df1[no_overlap] = df2[no_overlap]\n",
    "        # Then with overlap\n",
    "        for col in overlap_cols:\n",
    "            if strategy is HelpEnum.ARRAY:\n",
    "                df1[col] = merge_into_array(\n",
    "                    pd.DataFrame({'old': df1[col], 'new': df2[col]}),\n",
    "                    drop_duplicates=True)\n",
    "            elif strategy is HelpEnum.JSON_ARRAY:\n",
    "                df1[col] = merge_into_json_array(pd.DataFrame({'old': df1[col], 'new': df2[col]}), unwrap=True,\n",
    "                                                 drop_duplicates=True)\n",
    "            else:\n",
    "                # We only accept merging if there are no values together\n",
    "                valid = (df2[col].isnull() | df1[col].isnull()) | (df2[col] == df1[col])\n",
    "                if not valid.all():\n",
    "                    raise ValueError(\n",
    "                        \"Merging columns failed for column {col}. Appears twice, with conflicting data.\\n\"\n",
    "                        \"Data:{df1}\\n\"\n",
    "                        \"{df2}\"\n",
    "                            .format(col=col,\n",
    "                                    df1=df1[~valid].reset_index()[[\"shop\", \"location\", \"downloaded_on\", \"product_id\", col]],\n",
    "                                    df2=df2[~valid].reset_index()[[\"shop\", \"location\", \"downloaded_on\", \"product_id\", col]]),\n",
    "                        col\n",
    "                    )\n",
    "                else:\n",
    "                    \"\"\"\n",
    "                    Fuck pandas.\n",
    "                    If you don't do this mask inplace, it fails for timezoned timestamps. Somehow...\n",
    "                    Don't even ask about what's going on here. I wouldn't know.\n",
    "                    \"\"\"\n",
    "                    try:\n",
    "                        cop = df2[col].copy()\n",
    "                        cop.mask(cop.isnull(), df1[col], inplace=True)\n",
    "                        df1[col] = cop\n",
    "                    except ValueError:\n",
    "                        df1[col] = df2[col].mask(df2[col].isnull(), df1[col])\n",
    "    return df1\n",
    "\n",
    "def apply_regexes(data: pd.DataFrame, regexes: list, fail_limit: int=10, collate: bool=True):\n",
    "    \"\"\"\n",
    "    Apply regexes to the data, combining the extractions into a single DataFrame.\n",
    "    Use capturing groups with predefined names (see docs of create_promo).\n",
    "    Performs automatic price collation: The capturing groups 'xxx_int' and 'xxx_frac' will be combined into 'xxx' and\n",
    "    the original columns will be dropped, unless collate is set to False.\n",
    "    :param data: The data.\n",
    "    :param regexes: The regexes to apply.\n",
    "    :param fail_limit: The number of failed strings to print out. 0 means all failed lines will get printed.\n",
    "    Default is 10 because when Sentry is enabled we cannot log too many lines.\n",
    "    :param collate: Whether or not to automatically collate prices.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if len(regexes) == 0:\n",
    "        raise ValueError('Pass a list of regexes please!')\n",
    "        \n",
    "    extraction = pd.DataFrame()\n",
    "    if isinstance(data, pd.Series):\n",
    "        data = pd.DataFrame({'promos': data})\n",
    "    all_matches = data.isnull()\n",
    "    for regex in regexes:\n",
    "        regex = multiply_whitespace(regex=regex)\n",
    "        captures = (re.compile(regex).groups > 0)\n",
    "        for c in data.columns:\n",
    "            matches = data[c].str.match(regex).mask(data[c].isnull(), True).astype(bool)\n",
    "            all_matches[c] = all_matches[c] | matches\n",
    "            if captures:\n",
    "                e = data[c].str.extract(regex, expand=True)\n",
    "                try:\n",
    "                    extraction = concat_df_duplicate_cols(extraction, e, 'exclusive')\n",
    "                except ValueError as e:\n",
    "                    logger.exception(e.args[0])\n",
    "                    raise e\n",
    "    if not all_matches.all().all():\n",
    "        failed = data.stack()[~(all_matches.stack())].drop_duplicates().tolist()\n",
    "        if fail_limit != 0:\n",
    "            lines = failed[0:min(fail_limit, len(failed))]\n",
    "        else:\n",
    "            lines = failed\n",
    "\n",
    "        # Print as one line so that we have the faulty strings as is (easy to copy in regex 101).\n",
    "        logger.error('Not everything matches, listing ({}) unique failed lines:\\n{}'\n",
    "                     .format(len(lines), '\\n'.join(lines)))\n",
    "\n",
    "        unmatched = data.mask(all_matches, None)\n",
    "\n",
    "        def format_failures(b: pd.Series):\n",
    "            l = [c for c in b if c is not None]\n",
    "            if l:\n",
    "                return \"Could not match all strings. Unmatched:\\n{}\".format('\\n'.join(l))\n",
    "            else:\n",
    "                return None\n",
    "        extraction[\"errors\"] = unmatched.apply(format_failures, axis=1)\n",
    "\n",
    "    # Add the original promo strings\n",
    "    extraction['strings'] = data.promos\n",
    "    return extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "Load the promotion strings into a Dataframe. We only consider the first entry in the promo strings list as that is enough for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT shop, promo_string \n",
    "FROM promo_strings\n",
    "\"\"\"\n",
    "\n",
    "with pg.connect(uri) as connection:\n",
    "    promotions = pd.read_sql(query, connection)\n",
    "\n",
    "promotions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO add fitting regex for all the promo strings.\n",
    "regexes = [\n",
    "]\n",
    "extraction = apply_regexes(promotions['promo_string'], regexes)\n",
    "\n",
    "# Have a look at your output.\n",
    "extraction.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
